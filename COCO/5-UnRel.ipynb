{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fc159a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import scipy\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../Common/')\n",
    "from COCOWrapper import COCOWrapper\n",
    "from Dataset import ImageDataset, my_dataloader\n",
    "from ModelWrapper import ModelWrapper\n",
    "from ResNet import get_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad5c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/gregory/Datasets/unrel'\n",
    "\n",
    "modes = {'initial-tune': 'Baseline', 'spire': 'SPIRE', 'fs-3': 'FS'}\n",
    "trials = [0,1,2,3,4,5,6,7]\n",
    "\n",
    "anns = scipy.io.loadmat('{}/annotations.mat'.format(data_dir))['annotations']\n",
    "\n",
    "coco = COCOWrapper(mode = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88d29c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the image labels and locations for UnRel\n",
    "files = []\n",
    "label_dict = defaultdict(list)\n",
    "\n",
    "for ann in anns:\n",
    "    # len(o) = 4\n",
    "    # o[0] -> filename\n",
    "    # o[2][0][0][0][0] -> info about objects, their locations, and relationship\n",
    "    o = ann[0][0][0]\n",
    "    y = o[2][0][0][0][0]\n",
    "    \n",
    "    file = '{}/images/{}'.format(data_dir, o[0][0])\n",
    "    files.append(file)\n",
    "    \n",
    "    label_dict[y[0][0]].append(file)\n",
    "    label_dict[y[1][0]].append(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adabb049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bench' 'dog' 'skateboard' 'tie' 'truck']\n"
     ]
    }
   ],
   "source": [
    "# Find the common objects that SPIRE identified as 'main' for some SP\n",
    "with open('./2-Models/HPS/spire/spire.json', 'r') as f:\n",
    "    mains = json.load(f)\n",
    "\n",
    "tmp = []\n",
    "for main in mains:\n",
    "    tmp.append(main.replace('+', ' '))\n",
    "    \n",
    "mains = tmp\n",
    "    \n",
    "mains = np.intersect1d(mains, list(label_dict))\n",
    "print(mains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "749cd444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results per object\n",
      "\n",
      "Object:  bench\n",
      "Baseline 0.457 0.0655\n",
      "SPIRE 0.455 0.0472\n",
      "FS 0.452 0.0206\n",
      "\n",
      "Object:  dog\n",
      "Baseline 0.86 0.0161\n",
      "SPIRE 0.852 0.0095\n",
      "FS 0.84 0.0166\n",
      "\n",
      "Object:  skateboard\n",
      "Baseline 0.098 0.0534\n",
      "SPIRE 0.152 0.0433\n",
      "FS 0.139 0.0499\n",
      "\n",
      "Object:  tie\n",
      "Baseline 0.16 0.0573\n",
      "SPIRE 0.213 0.0662\n",
      "FS 0.188 0.0623\n",
      "\n",
      "Object:  truck\n",
      "Baseline 0.37 0.0585\n",
      "SPIRE 0.391 0.0531\n",
      "FS 0.363 0.0341\n",
      "\n",
      "\n",
      "Aggregated results\n",
      "\n",
      "Baseline 0.389\n",
      "SPIRE 0.413\n",
      "FS 0.396\n"
     ]
    }
   ],
   "source": [
    "# Setup the labels\n",
    "y = []\n",
    "for file in files:\n",
    "    y_tmp = np.zeros((91))\n",
    "    for main in mains:\n",
    "        index = coco.get_class_id(main)\n",
    "        if file in label_dict[main]:\n",
    "            y_tmp[index] = 1\n",
    "    y.append(y_tmp)\n",
    "\n",
    "# Get the model's predictions\n",
    "out = {}\n",
    "for mode in modes:\n",
    "    out_mode = defaultdict(list)\n",
    "    for trial in trials:  \n",
    "        model_dir = './2-Models/Models/{}/trial{}/model.pt'.format(mode, trial)\n",
    "\n",
    "        model, _ = get_model(mode = 'tune', parent = model_dir, out_features = 91)\n",
    "        model.eval()\n",
    "        model.cuda()\n",
    "\n",
    "        wrapper = ModelWrapper(model)\n",
    "\n",
    "        dataset = ImageDataset(files, y)\n",
    "        dataloader = my_dataloader(dataset)\n",
    "        y_hat, y_true = wrapper.predict_dataset(dataloader)\n",
    "        \n",
    "        for main in mains:\n",
    "            index = coco.get_class_id(main)\n",
    "            v = average_precision_score(y_true[:, index], y_hat[:, index])\n",
    "            out_mode[main].append(v)\n",
    "    out[mode] = out_mode    \n",
    "\n",
    "# Get results\n",
    "print('Results per object')\n",
    "for main in mains:\n",
    "    print()\n",
    "    print('Object: ', main)\n",
    "    for mode in modes:\n",
    "        v = out[mode][main]\n",
    "        print(modes[mode], np.round(np.mean(v), 3), np.round(np.std(v), 4))\n",
    "        \n",
    "print()\n",
    "print()\n",
    "print('Aggregated results')\n",
    "print()\n",
    "\n",
    "for mode in modes:\n",
    "    tmp = []\n",
    "    for main in mains:\n",
    "        tmp.append(np.mean(out[mode][main]))\n",
    "    print(modes[mode], np.round(np.mean(tmp), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa3cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:countervision]",
   "language": "python",
   "name": "conda-env-countervision-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
